import argparse
import json
import os
import sys
from glob import glob

from utils import *


def read_data(filename):
    pass


def train(args):
    train_path, dev_path, test_path = (os.path.join(dataset_path, file) for file in (train_file, dev_file, test_file))
    working_data = {"train": None}
    if not os.path.exists(dev_path):
        if training_args.do_eval:
            logger.warning(
                f"Evaluation data not found in {dev_path}. \
                Please input the correct path of evaluation data.\
                    Auto-training without evaluation data..."
            )
        training_args.do_eval = False
    else:
        working_data["dev"] = None
    if not os.path.exists(test_path):
        if training_args.do_predict:
            logger.warning(
                f"Testing data not found in {test_path}. \
                Please input the correct path of testing data.\
                    Auto-training without testing data..."
            )
        training_args.do_predict = False
    else:
        working_data["test"] = None

    if training_args.load_best_model_at_end and not training_args.do_eval:
        raise ValueError(
            "Cannot load best model at end when do_eval is False. Auto-adjust. "
            + "Please adjust load_best_model_at_end or do_eval."
        )

    # Model & Data Setup
    set_device(training_args.device)
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    model = UIE.from_pretrained(model_name_or_path)
    convert_function = partial(
        convert_and_tokenize_function,
        tokenizer=tokenizer,
        max_seq_len=max_seq_len,
    )
    for data in working_data:
        working_data[data] = load_dataset(
            read_data_by_chunk,
            data_path=eval(f"{data}_path"),
            max_seq_len=max_seq_len,
            lazy=False,
        )
        working_data[data] = working_data[data].map(convert_function)

    # Trainer Setup
    trainer = Trainer(
        model=model,
        criterion=criterion,
        args=training_args,
        train_dataset=working_data["train"] if training_args.do_train else None,
        eval_dataset=working_data["dev"] if training_args.do_eval else None,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        optimizers=optimizers,
    )
    trainer.optimizers = (
        optimizer.AdamW(learning_rate=training_args.learning_rate, parameters=model.parameters())
        if optimizers[0] is None
        else optimizers[0]
    )

    # Checkpoint Setup
    checkpoint, last_checkpoint = None, None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint

    # Start Training
    if training_args.do_train:
        train_result = trainer.train(resume_from_checkpoint=checkpoint)
        metrics = train_result.metrics
        trainer.save_model()
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()

    # Start Evaluate and tests model
    if training_args.do_eval:
        eval_metrics = trainer.evaluate()
        trainer.log_metrics("eval", eval_metrics)

    # Start Testing
    if training_args.do_predict:
        predict_output = trainer.predict(test_dataset=working_data["test"])
        trainer.log_metrics("test", predict_output.metrics)

    # export inference model
    if training_args.do_export:
        export_model_dir = export_model_dir if export_model_dir else training_args.output_dir
        export_model(
            model=trainer.model,
            input_spec=[
                InputSpec(shape=[None, None], dtype="int64", name="input_ids"),
                InputSpec(shape=[None, None], dtype="int64", name="token_type_ids"),
                InputSpec(shape=[None, None], dtype="int64", name="position_ids"),
                InputSpec(shape=[None, None], dtype="int64", name="attention_mask"),
            ],
            path=export_model_dir,
        )
        trainer.tokenizer.save_pretrained(export_model_dir)
    logger.info("Finish training.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--train_data", type=str, default=os.environ["SM_CHANNEL_TRAIN"])
    parser.add_argument("--validation_data", type=str, default=os.environ["SM_CHANNEL_VALIDATION"])
    parser.add_argument("--test_data", type=str, default=os.environ["SM_CHANNEL_TEST"])
    parser.add_argument("--output_dir", type=str, default=os.environ["SM_OUTPUT_DIR"])
    parser.add_argument("--num_gpus", type=int, default=os.environ["SM_NUM_GPUS"])

    parser.add_argument("--device", type=str, default="gpu")
    parser.add_argument("--seed", type=int, default=11)
    parser.add_argument("--epochs", type=float, default=0.5)
    parser.add_argument("--max_seq_len", type=int, default=512)
    args = parser.parse_args()
    train(args)

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
